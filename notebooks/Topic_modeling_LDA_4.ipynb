{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic modeling_LDA_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manoharsham/topic_model/blob/master/Topic_modeling_LDA_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "015s1NDe-IKc",
        "colab_type": "text"
      },
      "source": [
        "**Topic Modeling in Python**\n",
        "Its a method to extract hidden topics from a document/corpus and label certain parts of it. This helps to segment a document to certain topics for different users. It is helpful for summarization, improving customer support, SEO and many more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBQQqa5fbrPl",
        "colab_type": "code",
        "outputId": "c1274e80-e6af-4866-a3fb-5df2062aaaff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/manoharsham/topic_model.git manu_model\n",
        "\n",
        "#print(data[1])\n",
        "def load_data(data, documents_list, titles, related_items):\n",
        "  for item in data:\n",
        "    textdata = item['text']\n",
        "    title = item['title']\n",
        "    related_articles=item['related']\n",
        "    stripped_text = textdata.strip()\n",
        "    documents_list.append(stripped_text)\n",
        "    titles.append(title)\n",
        "    related_items.append(related_articles)\n",
        "\n",
        "  print(\"Total Number of Documents:\", len(documents_list))\n",
        "\n",
        "  return documents_list, titles, related_items\n",
        " \n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'manu_model' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXMqfMmjOUzM",
        "colab_type": "code",
        "outputId": "c0764e28-9d3d-44ce-814e-a6215d23e825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "import json\n",
        "import glob\n",
        "\n",
        "doclist = []\n",
        "titles = []\n",
        "related= []\n",
        "for file in glob.glob('manu_model/*files/AA/wiki_*'):\n",
        "  print(\"Processing\", file, \"...\")\n",
        "  data = [json.loads(line) for line in open(file, 'r')]\n",
        "  doclist, titles, related = load_data(data, doclist, titles, related)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing manu_model/Physicsfiles/AA/wiki_00 ...\n",
            "Total Number of Documents: 41\n",
            "Processing manu_model/NLPfiles/AA/wiki_01 ...\n",
            "Total Number of Documents: 183\n",
            "Processing manu_model/NLPfiles/AA/wiki_00 ...\n",
            "Total Number of Documents: 314\n",
            "Processing manu_model/AIfiles/AA/wiki_01 ...\n",
            "Total Number of Documents: 456\n",
            "Processing manu_model/AIfiles/AA/wiki_00 ...\n",
            "Total Number of Documents: 587\n",
            "Processing manu_model/sigprocfiles/AA/wiki_01 ...\n",
            "Total Number of Documents: 669\n",
            "Processing manu_model/sigprocfiles/AA/wiki_00 ...\n",
            "Total Number of Documents: 858\n",
            "Processing manu_model/CVfiles/AA/wiki_00 ...\n",
            "Total Number of Documents: 963\n",
            "Processing manu_model/speechfiles/AA/wiki_00 ...\n",
            "Total Number of Documents: 985\n",
            "Processing manu_model/telecomfiles/AA/wiki_00 ...\n",
            "Total Number of Documents: 1078\n",
            "Processing manu_model/MLfiles/AA/wiki_01 ...\n",
            "Total Number of Documents: 1166\n",
            "Processing manu_model/MLfiles/AA/wiki_00 ...\n",
            "Total Number of Documents: 1297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGkvX2jWCBip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import modules\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow8V0dD-Oipu",
        "colab_type": "code",
        "outputId": "62a08c1b-8978-46ed-a206-8a3ddb3fe035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(doclist))\n",
        "import numpy as np\n",
        "#indices = [i for i, x in enumerate(titles) if x == 'Neural Style Transfer']\n",
        "unique_idx = [titles.index(x) for x in set(titles)]\n",
        "titles_act=[titles[i] for i in unique_idx] \n",
        "doclist_act=[doclist[i] for i in unique_idx] \n",
        "related_act=[related[i] for i in unique_idx] \n",
        "\n",
        "titles=titles_act\n",
        "doclist=doclist_act\n",
        "related=related_act\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6oIOx1_1Dqe",
        "colab_type": "code",
        "outputId": "df2e44b2-a08e-48fd-8c1b-977707a55e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(len(doclist))\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "798\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk1cFfizEOXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractions_dict = {     \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I had\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"iit will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they had\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
        "                                      flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contractions_dict.get(match) \\\n",
        "            if contractions_dict.get(match) \\\n",
        "            else contractions_dict.get(match.lower())\n",
        "        expanded_contraction = expanded_contraction\n",
        "        return expanded_contraction\n",
        "\n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci_KHstnFIUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "from string import punctuation\n",
        "\n",
        "def preprocess_data(document):\n",
        "    \"\"\"\n",
        "    Input  : docuemnt list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text\n",
        "    \"\"\"\n",
        "    # initialize regex tokenizer\n",
        "    \n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "        # clean and tokenize document string\n",
        "    #raw = ''.join(document).lower()\n",
        "    document=expand_contractions(document.lower(),contractions_dict)\n",
        "    document=re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", document)\n",
        "    document = ''.join(c for c in document if c not in punctuation)\n",
        "\n",
        "    document=document.lower().replace('(','').replace(')','')\n",
        "    document=document.replace(',','').replace('.','').replace(':','').replace(';','')\n",
        "    raw = document.lower()\n",
        "    #tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    #tokens = tokenizer.tokenize(raw)\n",
        "    tokens = word_tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "    #texts.append(stemmed_tokens)\n",
        "    texts=stemmed_tokens\n",
        "    return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgrnFttuiWyd",
        "colab_type": "code",
        "outputId": "bf86ccbe-8a95-4db9-8faa-a220bb26cc78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(preprocess_data(doclist[300]))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lazi', 'learn', 'machin', 'learn', 'lazi', 'learn', 'learn', 'method', 'gener', 'train', 'data', 'theori', 'delay', 'queri', 'made', 'system', 'oppos', 'eager', 'learn', 'system', 'tri', 'gener', 'train', 'data', 'receiv', 'queri', 'primari', 'motiv', 'employ', 'lazi', 'learn', 'knearest', 'neighbor', 'algorithm', 'use', 'onlin', 'recommend', 'system', 'peopl', 'viewedpurchasedlisten', 'movieitemtun', 'also', 'data', 'set', 'continu', 'updat', 'new', 'entri', 'eg', 'new', 'item', 'sale', 'amazon', 'new', 'movi', 'view', 'netflix', 'new', 'clip', 'youtub', 'new', 'music', 'spotifi', 'pandora', 'continu', 'updat', 'train', 'data', 'would', 'render', 'obsolet', 'rel', 'short', 'time', 'especi', 'area', 'like', 'book', 'movi', 'new', 'bestsel', 'hit', 'moviesmus', 'publishedreleas', 'continu', 'therefor', 'one', 'realli', 'talk', 'train', 'phase', 'lazi', 'classifi', 'use', 'larg', 'continu', 'chang', 'dataset', 'attribut', 'commonli', 'queri', 'specif', 'even', 'larg', 'set', 'attribut', 'exist', 'exampl', 'book', 'year', 'public', 'author', 'publish', 'titl', 'edit', 'isbn', 'sell', 'price', 'etc', 'recommend', 'queri', 'reli', 'far', 'fewer', 'attribut', 'eg', 'purchas', 'view', 'cooccurr', 'data', 'user', 'rate', 'item', 'purchasedview', 'main', 'advantag', 'gain', 'employ', 'lazi', 'learn', 'method', 'target', 'function', 'approxim', 'local', 'knearest', 'neighbor', 'algorithm', 'target', 'function', 'approxim', 'local', 'queri', 'system', 'lazi', 'learn', 'system', 'simultan', 'solv', 'multipl', 'problem', 'deal', 'success', 'chang', 'problem', 'domain', 'said', 'advantag', 'system', 'achiev', 'predict', 'use', 'singl', 'train', 'set', 'develop', 'object', 'demonstr', 'case', 'knn', 'techniqu', 'instancebas', 'function', 'estim', 'local', 'theoret', 'disadvantag', 'lazi', 'learn', 'includ', 'larg', 'space', 'requir', 'store', 'entir', 'train', 'dataset', 'practic', 'issu', 'advanc', 'hardwar', 'rel', 'small', 'number', 'attribut', 'ega', 'cooccurr', 'frequenc', 'need', 'store', 'particularli', 'noisi', 'train', 'data', 'increas', 'case', 'base', 'unnecessarili', 'abstract', 'made', 'train', 'phase', 'practic', 'state', 'earlier', 'lazi', 'learn', 'appli', 'situat', 'learn', 'perform', 'advanc', 'soon', 'becom', 'obsolet', 'chang', 'data', 'also', 'problem', 'lazi', 'learn', 'optim', 'noisi', 'data', 'realli', 'occur', 'purchas', 'book', 'either', 'bought', 'anoth', 'book', 'lazi', 'learn', 'method', 'usual', 'slower', 'evalu', 'practic', 'larg', 'databas', 'high', 'concurr', 'load', 'queri', 'postpon', 'actual', 'queri', 'time', 'recomput', 'advanc', 'period', 'basi', 'eg', 'nightli', 'anticip', 'futur', 'queri', 'answer', 'store', 'way', 'next', 'time', 'new', 'queri', 'ask', 'exist', 'entri', 'databas', 'answer', 'mere', 'look', 'rapidli', 'instead', 'comput', 'fli', 'would', 'almost', 'certainli', 'bring', 'highconcurr', 'multius', 'system', 'knee', 'larger', 'train', 'data', 'also', 'entail', 'increas', 'cost', 'particularli', 'fix', 'amount', 'comput', 'cost', 'processor', 'process', 'limit', 'amount', 'train', 'data', 'point', 'standard', 'techniqu', 'improv', 'recomput', 'effici', 'particular', 'answer', 'recomput', 'unless', 'data', 'impact', 'answer', 'chang', 'eg', 'new', 'item', 'new', 'purchas', 'new', 'view', 'word', 'store', 'answer', 'updat', 'increment', 'approach', 'use', 'larg', 'ecommerc', 'media', 'site', 'long', 'use', 'entrez', 'portal', 'nation', 'center', 'biotechnolog', 'inform', 'ncbi', 'precomput', 'similar', 'differ', 'item', 'larg', 'dataset', 'biolog', 'sequenc', 'protein', 'structur', 'publishedarticl', 'abstract', 'etc', 'find', 'similar', 'queri', 'ask', 'frequent', 'ncbi', 'use', 'highli', 'parallel', 'hardwar', 'perform', 'nightli', 'recomput', 'recomput', 'perform', 'new', 'entri', 'dataset', 'exist', 'entri', 'similar', 'two', 'exist', 'entri', 'need', 'recomput', 'knearest', 'neighbor', 'special', 'case', 'instancebas', 'learn', 'local', 'regress', 'lazi', 'naiv', 'bay', 'rule', 'extens', 'use', 'commerci', 'spam', 'detect', 'softwar', 'spammer', 'keep', 'get', 'smarter', 'revis', 'spam', 'strategi', 'therefor', 'learn', 'rule', 'must', 'also', 'continu', 'updat', 'lazi', 'lazi', 'learn', 'local', 'regress', 'r', 'packag', 'refer', 'manual', 'webb', 'gi', 'lazi', 'learn', 'sammut', 'c', 'webb', 'gi', 'ed', 'encyclopedia', 'machin', 'learn', 'springer', 'boston']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xn_9dmAUuMEd",
        "colab": {}
      },
      "source": [
        "from scipy.stats import entropy\n",
        "def jensen_shannon(p, q):\n",
        "  #  p = query[None,:].T \n",
        "  #  q = matrix.T\n",
        "   # print(p.shape)\n",
        "   # print(q.shape)\n",
        "    m = 0.5*(p + q)\n",
        "\n",
        "    return np.sqrt(np.max([0,0.5*(entropy(p,m) + entropy(q,m))]))\n",
        "def get_most_similar_documents(query,matrix,k=10): #k=10 it will recommend top 10 documents simliar to the given document\n",
        "    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
        "    return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWcFW-Eyv3NN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "#print(preprocess_data(doclist[1]))\n",
        "doclist_list=[preprocess_data(doclist[i]) for i in range(len(doclist))]\n",
        "dictionary = Dictionary(doclist_list)\n",
        "corpus = [dictionary.doc2bow(text) for text in doclist_list]\n",
        "lda = LdaModel(corpus, num_topics=20, minimum_probability=0.0, id2word=dictionary, passes=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgSMi_QRd6uR",
        "colab_type": "code",
        "outputId": "26f89489-8e84-4a6f-b4aa-b61d65dcbe9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import random\n",
        "ref_id=random.randint(0,len(titles));\n",
        "print(ref_id)\n",
        "testdoc=preprocess_data(doclist[ref_id])\n",
        "test_bow = dictionary.doc2bow(testdoc)\n",
        "test_doc_dist = np.array([tup[1] for tup in lda.get_document_topics(bow=test_bow)])\n",
        "print(test_doc_dist)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "417\n",
            "[6.4020489e-05 6.4020489e-05 3.0832350e-01 6.4020489e-05 6.4020489e-05\n",
            " 6.5958843e-02 6.4020489e-05 1.2343487e-01 6.4020489e-05 6.4020489e-05\n",
            " 6.4020489e-05 6.4020489e-05 6.4020489e-05 6.4020489e-05 3.0293569e-03\n",
            " 6.4020489e-05 4.9829313e-01 6.4020489e-05 6.4020489e-05 6.4020489e-05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvmtpWKK3xLv",
        "colab_type": "code",
        "outputId": "a0e9d53e-874b-450f-e2a7-8d4e23757eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import pickle\n",
        "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "dictionary.save('dictionary.gensim')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCjq6Mm_642x",
        "colab_type": "code",
        "outputId": "3dd303f4-2b34-4ee1-91e2-413d1796140e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "#ref_id=random.randint(0,len(titles));\n",
        "#ref_id=774\n",
        "#fields = ['ref doc', 'predicted see also', 'predicted metric', 'wiki see also','predicted metric for wiki']  \n",
        "fields = ['ref doc', 'predicted see also[JS]', 'predicted metric[JS]', 'predicted see also[HM]', 'predicted metric[HM]', 'wiki see also','predicted metric for wiki[JS]','predicted metric for wiki[HM]']  \n",
        "\n",
        "resultsDict={}\n",
        "docvecs=[]\n",
        "docnames=[]\n",
        "titles1=np.array(titles)\n",
        "from gensim.matutils import hellinger\n",
        "\n",
        "for ref_id in range(len(titles1)):\n",
        "  testdoc=preprocess_data(doclist[ref_id])\n",
        "  test_bow = dictionary.doc2bow(testdoc)\n",
        "  test_doc_dist = np.array([tup[1] for tup in lda.get_document_topics(bow=test_bow)])\n",
        "  docvecs.append(test_doc_dist)\n",
        "  docnames.append(titles1[ref_id])\n",
        "for ref_id in range(len(titles1)):\n",
        "  jssim_vec=[]\n",
        "  hellsim_vec=[]\n",
        "  print(ref_id)\n",
        "  for docid in range(len(titles1)):\n",
        "    if(docid==ref_id):\n",
        "      #docvecs.append(test_doc_dist)\n",
        "      #docnames.append(titles[docid])\n",
        "      jssim_vec.append(1000)\n",
        "      hellsim_vec.append(1000)\n",
        "      continue\n",
        "    jssim=jensen_shannon(docvecs[docid],docvecs[ref_id])\n",
        "    hellsim_vec.append(hellinger(docvecs[docid],docvecs[ref_id]))\n",
        "    #if(docid==ref_id):\n",
        "    # print('MANOHAR-->',jssim)\n",
        "    #  print(test_doc_dist)\n",
        "    #  print(test_doc_dist_new)\n",
        "    #  pp=test_doc_dist\n",
        "    #  qq=test_doc_dist_new\n",
        "    #  mm = 0.5*(pp + qq)\n",
        "    #  #print('MANU')\n",
        "      #print(entropy(pp,qq))\n",
        "    #  new_res=np.sqrt(0.5*(entropy(pp,mm) + entropy(qq,mm)))\n",
        "    #  #print(new_res)\n",
        "    jssim_vec.append(jssim)\n",
        "  #plt.plot(jssim_vec)\n",
        "  #plt.show\n",
        "#print(doclist[0])\n",
        "  jssim_vec=np.array(jssim_vec)\n",
        "  hellsim_vec=np.array(hellsim_vec)\n",
        "\n",
        "  top5idx=np.argpartition(jssim_vec, 5)\n",
        "  top5idx_hm=np.argpartition(hellsim_vec, 5)\n",
        "  titles1=np.array(titles)\n",
        "  related1=np.array(related)\n",
        "  #print('Reference document title--->',titles1[ref_id])\n",
        "  resultsDict.setdefault(fields[0], []).append(titles1[ref_id])\n",
        "  #print('Top five similar documents-->',titles1[top5idx[:5]])\n",
        "  resultsDict.setdefault(fields[1], []).append(titles1[top5idx[:5]])\n",
        "  #print('Jenson Shannon similarity of top 5 similar topics to reference document:-->')\n",
        "  #print(jssim_vec[top5idx[:5]])\n",
        "  resultsDict.setdefault(fields[2], []).append(jssim_vec[top5idx[:5]])\n",
        "  resultsDict.setdefault(fields[3], []).append(titles1[top5idx_hm[:5]])\n",
        "  #print('Jenson Shannon similarity of top 5 similar topics to reference document:-->')\n",
        "  #print(jssim_vec[top5idx[:5]])\n",
        "  resultsDict.setdefault(fields[4], []).append(jssim_vec[top5idx_hm[:5]])\n",
        "\n",
        "\n",
        "  U=[S[2:] for S in related1[ref_id].split('\\n')]\n",
        "  U=U[1:-1]\n",
        "  if '' in U:\n",
        "    U=U[:U.index('')]\n",
        "  #print('Top five related documents from Wiki-->',U)\n",
        "  overlap_idx=[]\n",
        "  wikiseenames=[]\n",
        "  wikiseemetrics=[]\n",
        "  wikiseemetrics_hm=[]\n",
        "\n",
        "  for ind in range(len(U)):\n",
        "    for ind2 in range(len(doclist)):\n",
        "      if(U[ind]==titles[ind2]):\n",
        "        overlap_idx.append(ind2)\n",
        "        wikiseenames.append(titles[ind2])\n",
        "        wikiseemetrics.append(jssim_vec[ind2])\n",
        "        wikiseemetrics_hm.append(hellsim_vec[ind2])\n",
        "        #print(titles1[ind2])\n",
        "        #print(jssim_vec[ind2])\n",
        "#for i in test_doc_dist.argsort()[::-1][:7]:\n",
        "#    print(i,lda.show_topic(topicid=i,topn=10),\"\\n\")\n",
        "# field names  \n",
        "  resultsDict.setdefault(fields[5], []).append(wikiseenames)\n",
        "  resultsDict.setdefault(fields[6], []).append(wikiseemetrics)\n",
        "  resultsDict.setdefault(fields[7], []).append(wikiseemetrics_hm)\n",
        "\n",
        "  #print(resultsDict)\n",
        "# name of csv file  \n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzQ2QfTcln7w",
        "colab_type": "code",
        "outputId": "6b661f8e-9ec8-4e6e-8be3-46a04a8d4c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "dictlist = [dict() for x in range(len(titles1))]\n",
        "#dictlist = [{fields[0]:resultsDict[fields[0]][i],fields[1]:resultsDict[fields[1]][i],fields[2]:resultsDict[fields[2]][i],fields[3]:resultsDict[fields[3]][i], fields[4]:resultsDict[fields[4]][i]} for i in range(len(titles1))]\n",
        "dictlist = [{fields[0]:resultsDict[fields[0]][i],fields[1]:resultsDict[fields[1]][i],fields[2]:resultsDict[fields[2]][i],fields[3]:resultsDict[fields[3]][i], fields[4]:resultsDict[fields[4]][i], fields[5]:resultsDict[fields[5]][i], fields[6]:resultsDict[fields[6]][i], fields[7]:resultsDict[fields[7]][i]} for i in range(len(titles1))]\n",
        "\n",
        "#print(dictlist)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import csv\n",
        "filename = \"results_NT20_P40_1.csv\"\n",
        "    \n",
        "\n",
        "    \n",
        "# writing to csv file  \n",
        "with open(filename, 'w') as csvfile:  \n",
        "    # creating a csv dict writer object  \n",
        "    writer = csv.DictWriter(csvfile, fieldnames = fields)  \n",
        "        \n",
        "    # writing headers (field names)  \n",
        "    writer.writeheader()  \n",
        "    print('Manohar')    \n",
        "    # writing data rows \n",
        "    for data in dictlist:\n",
        "      writer.writerow(data)\n",
        "!cp results_NT20_P40_1.csv \"/content/gdrive/My Drive/\"\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Manohar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBgLyjMOnGEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "e3a83cc8-d951-4d27-c37a-afeb4a15beaa"
      },
      "source": [
        "print(ref_id)\n",
        "print(top5idx[:5])\n",
        "from matplotlib.pyplot import stem\n",
        "\n",
        "#stem(y, linefmt='b-', markerfmt='bo', basefmt='r-')\n",
        "\n",
        "stem(docvecs[ref_id],linefmt='r-')\n",
        "print(docvecs[ref_id])\n",
        "jj=0\n",
        "mf=['bo','rx','ks','b>','mo','gs']\n",
        "for docid in top5idx[:5]:\n",
        "  stem(docvecs[docid],markerfmt=mf[jj])\n",
        "  jj=jj+1\n",
        "  print(docnames[docid])\n",
        "stem(docvecs[648])\n",
        "#plt.show()\n",
        "print(docnames[648])\n",
        "\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "797\n",
            "[525 410 401 671 312]\n",
            "[2.2133690e-05 2.2133690e-05 8.9220172e-03 2.2133690e-05 2.2133690e-05\n",
            " 2.2133690e-05 5.9850013e-01 2.0853832e-01 2.2133690e-05 2.2133690e-05\n",
            " 2.2133690e-05 1.5988803e-02 2.2133690e-05 1.3037340e-01 2.2133690e-05\n",
            " 2.2133690e-05 1.3556057e-02 2.2133690e-05 2.3833549e-02 2.2133690e-05]\n",
            "In-phase and quadrature components\n",
            "Lanczos resampling\n",
            "Pulse (signal processing)\n",
            "Hilbert spectral analysis\n",
            "Pulse-width modulation\n",
            "Dependent component analysis\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}